{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETL Complaint Facts\n",
    "# If using the native Google BigQuery API module:\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.exceptions import NotFound\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "from google.oauth2 import service_account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the GCP Project, dataset and table name\n",
    "gcp_project = 'cis-4400-404715'\n",
    "path_to_service_account_key_file = 'keys.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_bigquery_table(bqclient, table_path, write_disposition, df):\n",
    "    \"\"\"\n",
    "    upload_bigquery_table\n",
    "    Accepts a path to a BigQuery table, the write disposition and a dataframe\n",
    "    Loads the data into the BigQuery table from the dataframe.\n",
    "    for credentials.\n",
    "    The write disposition is either\n",
    "    write_disposition=\"WRITE_TRUNCATE\"  Erase the target data and load all new data.\n",
    "    write_disposition=\"WRITE_APPEND\"    Append to the existing table\n",
    "    \"\"\"\n",
    "    try:\n",
    "        \n",
    "        job_config = bigquery.LoadJobConfig(write_disposition=write_disposition)\n",
    "        \n",
    "        # Submit the job\n",
    "        job = bqclient.load_table_from_dataframe(df, table_path, job_config=job_config)\n",
    "        \n",
    "        # Show the job results\n",
    "        job.result()\n",
    "    except Exception as err:\n",
    "        print(\"Failed to load BigQuery Table.\", err)\n",
    "        # os._exit(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigquery_table_exists(table_path, bqclient):\n",
    "    \"\"\"\n",
    "    bigquery_table_exists\n",
    "    Accepts a path to a BigQuery table\n",
    "    Checks if the BigQuery table exists.\n",
    "    Returns True or False\n",
    "    \"\"\"\n",
    "    try:\n",
    "        bqclient.get_table(table_path)  # Make an API request.\n",
    "        return True\n",
    "    except NotFound:\n",
    "        # print(\"Table {} is not found.\".format(table_id))\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_new_table(bqclient, table_path, df):\n",
    "    \"\"\"\n",
    "    build_new_table\n",
    "    Accepts a path to a dimensional table, the dimension name and a data frame\n",
    "    Add the surrogate key and a record timestamp to the data frame\n",
    "    Inserts the contents of the dataframe to the dimensional table.\n",
    "    \"\"\"\n",
    "    upload_bigquery_table(bqclient, table_path, \"WRITE_TRUNCATE\", df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_existing_table( bqclient, table_path, df):\n",
    "    \"\"\"\n",
    "    insert_existing_table\n",
    "    Accepts a path to a dimensional table, the dimension name and a data frame\n",
    "    Compares the new data to the existing data in the table.\n",
    "    Inserts the new/modified records to the existing table\n",
    "    \"\"\"\n",
    "    upload_bigquery_table( bqclient, table_path, \"WRITE_APPEND\", df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_bigquery_table(table_path, bqclient, surrogate_key):\n",
    "    \"\"\"\n",
    "    query_bigquery_table\n",
    "    Accepts a path to a BigQuery table and the name of the surrogate key\n",
    "    Queries the BigQuery table but leaves out the update_timestamp and surrogate key columns\n",
    "    Returns the dataframe\n",
    "    \"\"\"    \n",
    "    bq_df = pd.DataFrame\n",
    "    # sql_query = 'SELECT * EXCEPT ( update_timestamp, '+surrogate_key+') FROM `' + table_path + '`'\n",
    "    sql_query = 'SELECT * FROM `' + table_path + '`'\n",
    "    bq_df = bqclient.query(sql_query).to_dataframe()\n",
    "    return bq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dimension_lookup(dimension_name, lookup_columns, df):\n",
    "    \"\"\"\n",
    "    dimension_lookup\n",
    "    Lookup the lookup_columns in the dimension_name and return the associated surrogate keys\n",
    "    Returns dataframe augmented with the surrogate keys\n",
    "    \"\"\"\n",
    "    bq_df = pd.DataFrame\n",
    "    surrogate_key = dimension_name+\"_dim_id\"\n",
    "    dimension_table_path = \".\".join([gcp_project,bq_dataset,dimension_name+\"_dimension\"])\n",
    "    # Fetch the existing table\n",
    "    bq_df = query_bigquery_table(dimension_table_path, bqclient, surrogate_key)\n",
    "    if dimension_name == 'date':\n",
    "        bq_df['full_date'] = bq_df['full_date'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "\n",
    "    print(bq_df)\n",
    "    # Melt the dimension dataframe into an index with the lookup columns\n",
    "    m = bq_df.melt(id_vars=lookup_columns, value_vars=surrogate_key)\n",
    "    print(m)\n",
    "    # Rename the \"value\" column to the surrogate key column name\n",
    "    m=m.rename(columns={\"value\":surrogate_key})\n",
    "    # Merge with the fact table record\n",
    "    df = df.merge(m, on=lookup_columns, how='left')\n",
    "    # Drop the \"variable\" column and the lookup columns\n",
    "    df = df.drop(columns=lookup_columns)\n",
    "    df = df.drop(columns=\"variable\")\n",
    "    #print(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_column(df, bq_dataset, dimension_name):\n",
    "\n",
    "    # Renaming for 311\n",
    "    if bq_dataset == '311_illegal_parking':\n",
    "        if dimension_name == 'complaint':\n",
    "            df = df.rename(columns={'descriptor': 'complaint_description'})\n",
    "        elif dimension_name == 'complaint_source':\n",
    "            df = df.rename(columns={'open_data_channel_type': 'complaint_source_channel'})\n",
    "        elif dimension_name == 'location':\n",
    "            df = df.rename(columns={'city': 'incident_city', 'incident_zip': 'incident_zipcode'})\n",
    "        elif dimension_name == 'date':\n",
    "            df = df.rename(columns={'created_date': 'full_date'})\n",
    "\n",
    "    # Renaming for Open Parking\n",
    "    elif bq_dataset == 'open_parking':\n",
    "        if dimension_name == 'agency':\n",
    "            df = df.rename(columns={'issuing_agency': 'agency_name'})\n",
    "        elif dimension_name == 'date':\n",
    "            df = df.rename(columns={'issue_date': 'full_date'})\n",
    "        elif dimension_name == 'location':\n",
    "            df = df.rename(columns={'precinct': 'precinct_num', 'county': 'borough', 'zipcode': 'incident_zipcode'})\n",
    "        elif dimension_name == 'violation':\n",
    "            df = df.rename(columns={'violation': 'violation_description'})\n",
    "        elif dimension_name == 'violator':\n",
    "            df = df.rename(columns={'plate': 'violator_plate', 'state': 'violator_state'})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_null_values(df, bq_dataset, dimension_name):\n",
    "    # Renaming for 311\n",
    "    if bq_dataset == '311_illegal_parking':\n",
    "      if dimension_name == 'location':\n",
    "        default_values = {\n",
    "          'city': 'Unspecified',\n",
    "          'incident_zip': 0,\n",
    "          'borough': 'Unspecified'\n",
    "        }\n",
    "        df.fillna(default_values, inplace=True)\n",
    "        \n",
    "    # Renaming for Open Parking\n",
    "    elif bq_dataset == 'open_parking':\n",
    "      if dimension_name == 'agency':\n",
    "        default_values = {\n",
    "          'issuing_agency': 'N/A'\n",
    "        }\n",
    "        df.fillna(default_values, inplace=True)\n",
    "\n",
    "      elif dimension_name == 'violation':\n",
    "        default_values = {\n",
    "          'violation_status': 'N/A'\n",
    "        }\n",
    "        df.fillna(default_values, inplace=True)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_location_attributes(df):\n",
    "    precinct_to_zipcode = {\n",
    "        1: 10013,\n",
    "        5: 10013,\n",
    "        6: 10014,\n",
    "        7: 10002,\n",
    "        9: 10003,\n",
    "        10: 10011,\n",
    "        13: 10010,\n",
    "        14: 10001,\n",
    "        17: 10022,\n",
    "        18: 10019,\n",
    "        19: 10065,\n",
    "        20: 10024,\n",
    "        22: 10024,\n",
    "        23: 10029,\n",
    "        24: 10025,\n",
    "        25: 10035,\n",
    "        26: 10027,\n",
    "        28: 10027,\n",
    "        30: 10031,\n",
    "        32: 10030,\n",
    "        33: 10032,\n",
    "        34: 10033,\n",
    "        40: 10454,\n",
    "        41: 10459,\n",
    "        42: 10451,\n",
    "        43: 10473,\n",
    "        44: 10452,\n",
    "        45: 10465,\n",
    "        46: 10457,\n",
    "        47: 10466,\n",
    "        48: 10457,\n",
    "        49: 10461,\n",
    "        50: 10463,\n",
    "        52: 10467,\n",
    "        60: 11224,\n",
    "        61: 11223,\n",
    "        62: 11214,\n",
    "        63: 11210,\n",
    "        66: 11204,\n",
    "        67: 11226,\n",
    "        68: 11220,\n",
    "        69: 11236,\n",
    "        70: 11230,\n",
    "        71: 11225,\n",
    "        72: 11232,\n",
    "        73: 11212,\n",
    "        75: 11208,\n",
    "        76: 11231,\n",
    "        77: 11213,\n",
    "        78: 11217,\n",
    "        79: 11216,\n",
    "        81: 11221,\n",
    "        83: 11237,\n",
    "        84: 11201,\n",
    "        88: 11205,\n",
    "        90: 11211,\n",
    "        94: 11222,\n",
    "        100: 11693,\n",
    "        101: 11691,\n",
    "        102: 11418,\n",
    "        103: 11432,\n",
    "        104: 11385,\n",
    "        105: 11428,\n",
    "        106: 11417,\n",
    "        107: 11365,\n",
    "        108: 11101,\n",
    "        109: 11354,\n",
    "        110: 11373,\n",
    "        111: 11361,\n",
    "        112: 11375,\n",
    "        113: 11434,\n",
    "        114: 11103,\n",
    "        115: 11372,\n",
    "        120: 10301,\n",
    "        121: 10314,\n",
    "        122: 10306,\n",
    "        123: 10307\n",
    "    }\n",
    "\n",
    "    zipcode_to_borough = {\n",
    "        10013: \"Manhattan\",\n",
    "        10014: \"Manhattan\",\n",
    "        10002: \"Manhattan\",\n",
    "        10003: \"Manhattan\",\n",
    "        10011: \"Manhattan\",\n",
    "        10010: \"Manhattan\",\n",
    "        10001: \"Manhattan\",\n",
    "        10022: \"Manhattan\",\n",
    "        10019: \"Manhattan\",\n",
    "        10065: \"Manhattan\",\n",
    "        10024: \"Manhattan\",\n",
    "        10029: \"Manhattan\",\n",
    "        10025: \"Manhattan\",\n",
    "        10035: \"Manhattan\",\n",
    "        10027: \"Manhattan\",\n",
    "        10031: \"Manhattan\",\n",
    "        10030: \"Manhattan\",\n",
    "        10032: \"Manhattan\",\n",
    "        10033: \"Manhattan\",\n",
    "        10454: \"Bronx\",\n",
    "        10459: \"Bronx\",\n",
    "        10451: \"Bronx\",\n",
    "        10473: \"Bronx\",\n",
    "        10452: \"Bronx\",\n",
    "        10465: \"Bronx\",\n",
    "        10457: \"Bronx\",\n",
    "        10466: \"Bronx\",\n",
    "        10461: \"Bronx\",\n",
    "        10463: \"Bronx\",\n",
    "        10467: \"Bronx\",\n",
    "        11224: \"Brooklyn\",\n",
    "        11223: \"Brooklyn\",\n",
    "        11214: \"Brooklyn\",\n",
    "        11210: \"Brooklyn\",\n",
    "        11204: \"Brooklyn\",\n",
    "        11226: \"Brooklyn\",\n",
    "        11220: \"Brooklyn\",\n",
    "        11236: \"Brooklyn\",\n",
    "        11230: \"Brooklyn\",\n",
    "        11225: \"Brooklyn\",\n",
    "        11232: \"Brooklyn\",\n",
    "        11212: \"Brooklyn\",\n",
    "        11208: \"Brooklyn\",\n",
    "        11231: \"Brooklyn\",\n",
    "        11213: \"Brooklyn\",\n",
    "        11217: \"Brooklyn\",\n",
    "        11216: \"Brooklyn\",\n",
    "        11221: \"Brooklyn\",\n",
    "        11237: \"Brooklyn\",\n",
    "        11201: \"Brooklyn\",\n",
    "        11205: \"Brooklyn\",\n",
    "        11211: \"Brooklyn\",\n",
    "        11222: \"Brooklyn\",\n",
    "        11693: \"Queens\",\n",
    "        11691: \"Queens\",\n",
    "        11418: \"Queens\",\n",
    "        11432: \"Queens\",\n",
    "        11385: \"Queens\",\n",
    "        11428: \"Queens\",\n",
    "        11417: \"Queens\",\n",
    "        11365: \"Queens\",\n",
    "        11101: \"Queens\",\n",
    "        11354: \"Queens\",\n",
    "        11373: \"Queens\",\n",
    "        11361: \"Queens\",\n",
    "        11375: \"Queens\",\n",
    "        11434: \"Queens\",\n",
    "        11103: \"Queens\",\n",
    "        11372: \"Queens\",\n",
    "        10301: \"Staten Island\",\n",
    "        10314: \"Staten Island\",\n",
    "        10306: \"Staten Island\",\n",
    "        10307: \"Staten Island\"\n",
    "    }\n",
    "\n",
    "    df['incident_zipcode'] = df['precinct_num'].map(precinct_to_zipcode)\n",
    "    df['borough'] = df['incident_zipcode'].map(zipcode_to_borough)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create fact table for 311 illegal parking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    df = pd.DataFrame\n",
    "    # Create the BigQuery Client\n",
    "    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = path_to_service_account_key_file\n",
    "\n",
    "    # Construct a BigQuery client object\n",
    "    bqclient = bigquery.Client()\n",
    "    \n",
    "    bq_dataset = '311_illegal_parking'\n",
    "    fact_name = '311_illegal_parking'\n",
    "    table_name = fact_name + '_fact'\n",
    "    # Construct the full BigQuery path to the table\n",
    "    fact_table_path = \".\".join([gcp_project,bq_dataset,table_name])\n",
    "    file_source_path = 'data/311_master.csv'\n",
    "\n",
    "    # Load in the data file\n",
    "    with open(file_source_path, 'r') as data:\n",
    "            df = pd.read_csv(data)\n",
    "   \n",
    "    # Set all of the column names to lower case letters\n",
    "    df = df.rename(columns=str.lower)\n",
    "\n",
    "    df = rename_column(df, bq_dataset, 'complaint')\n",
    "    df = dimension_lookup(dimension_name='complaint', lookup_columns=['complaint_type', 'complaint_description'], df=df)\n",
    "\n",
    "    df = rename_column(df, bq_dataset, 'complaint_source') \n",
    "    df = dimension_lookup(dimension_name='complaint_source', lookup_columns=['complaint_source_channel'], df=df)\n",
    "\n",
    "    df = rename_column(df, bq_dataset, 'date') \n",
    "    df['full_date'] = pd.to_datetime(df['full_date'])\n",
    "    df['year'] = df['full_date'].dt.year\n",
    "    df['month'] = df['full_date'].dt.month\n",
    "    df['month_name'] = df['full_date'].dt.strftime('%B')\n",
    "    df['day'] = df['full_date'].dt.day\n",
    "    df['weekday_name'] = df['full_date'].dt.strftime('%A')\n",
    "    df['full_date'] = df['full_date'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "    df = dimension_lookup(dimension_name='date', lookup_columns=['full_date', 'year', 'month', 'month_name', 'day', 'weekday_name'], df=df)\n",
    "\n",
    "    df = handle_null_values(df, bq_dataset, 'location')\n",
    "    df = rename_column(df, bq_dataset, 'location') \n",
    "    df = dimension_lookup(dimension_name='location', lookup_columns=['borough', 'incident_city', 'incident_zipcode'], df=df)\n",
    "\n",
    "    df = rename_column(df, bq_dataset, 'status') \n",
    "    df = dimension_lookup(dimension_name='status', lookup_columns=['status'], df=df)\n",
    "\n",
    "    # A list of all of the surrogate keys\n",
    "    # For transaction grain, also include the 'unique_key' column\n",
    "    surrogate_keys=['unique_key', 'complaint_dim_id','complaint_source_dim_id','date_dim_id','location_dim_id','status_dim_id']\n",
    "    \n",
    "    # Remove all of the other non-surrogate key columns\n",
    "    df = df[surrogate_keys]\n",
    "\n",
    "    # See if the target table exists\n",
    "    target_table_exists = bigquery_table_exists(fact_table_path, bqclient )\n",
    "    # If the target table does not exist, load all of the data into a new table\n",
    "    if not target_table_exists:\n",
    "        build_new_table( bqclient, fact_table_path, df)\n",
    "    # If the target table exists, then perform an incremental load\n",
    "    if target_table_exists:\n",
    "        insert_existing_table( bqclient, fact_table_path, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create fact table for Open Parking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    df = pd.DataFrame\n",
    "    # Create the BigQuery Client\n",
    "    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = path_to_service_account_key_file\n",
    "\n",
    "    # Construct a BigQuery client object\n",
    "    bqclient = bigquery.Client()\n",
    "\n",
    "    bq_dataset = 'open_parking'\n",
    "    fact_name = 'open_parking'\n",
    "    table_name = fact_name + '_fact'\n",
    "    # Construct the full BigQuery path to the table\n",
    "    fact_table_path = \".\".join([gcp_project,bq_dataset,table_name])\n",
    "    file_source_path = 'data/open_parking_master.csv'\n",
    "\n",
    "    # Load in the data file\n",
    "    with open(file_source_path, 'r') as data:\n",
    "            df = pd.read_csv(data)\n",
    "\n",
    "    # Set all of the column names to lower case letters\n",
    "    df = df.rename(columns=str.lower)\n",
    "\n",
    "    df = rename_column(df, bq_dataset, 'agency')\n",
    "    df = dimension_lookup(dimension_name='agency', lookup_columns=['agency_name'], df=df)\n",
    "\n",
    "    df = rename_column(df, bq_dataset, 'date') \n",
    "    df['full_date'] = pd.to_datetime(df['full_date'], format='%m/%d/%Y', errors='coerce')\n",
    "    df = df.dropna(subset=['full_date'], axis=0)\n",
    "    df['year'] = df['full_date'].dt.year\n",
    "    df['month'] = df['full_date'].dt.month\n",
    "    df['month_name'] = df['full_date'].dt.strftime('%B')\n",
    "    df['day'] = df['full_date'].dt.day\n",
    "    df['weekday_name'] = df['full_date'].dt.strftime('%A')\n",
    "    df['full_date'] = df['full_date'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "    df = dimension_lookup(dimension_name='date', lookup_columns=['full_date', 'year', 'month', 'month_name', 'day', 'weekday_name'], df=df)\n",
    "\n",
    "    df = rename_column(df, bq_dataset, 'location')\n",
    "    df = calculate_location_attributes(df)\n",
    "    df = dimension_lookup(dimension_name='location', lookup_columns=['precinct_num', 'borough', 'incident_zipcode'], df=df)\n",
    "\n",
    "    df = handle_null_values(df, bq_dataset, 'violation')\n",
    "    df = rename_column(df, bq_dataset, 'violation') \n",
    "    df = dimension_lookup(dimension_name='violation', lookup_columns=['violation_description', 'violation_status'], df=df)\n",
    "\n",
    "    df = rename_column(df, bq_dataset, 'violator') \n",
    "    df = dimension_lookup(dimension_name='violator', lookup_columns=['violator_state', 'license_type'], df=df)\n",
    "\n",
    "    # A list of all of the surrogate keys\n",
    "    # For transaction grain, also include the 'unique_key' column\n",
    "    surrogate_keys=['summons_number', 'agency_dim_id', 'location_dim_id', 'date_dim_id', 'violation_dim_id', 'violator_dim_id']\n",
    "    \n",
    "    # Remove all of the other non-surrogate key columns\n",
    "    df = df[surrogate_keys]\n",
    "\n",
    "    # See if the target table exists\n",
    "    target_table_exists = bigquery_table_exists(fact_table_path, bqclient )\n",
    "    # If the target table does not exist, load all of the data into a new table\n",
    "    if not target_table_exists:\n",
    "        build_new_table( bqclient, fact_table_path, df)\n",
    "    # If the target table exists, then perform an incremental load\n",
    "    if target_table_exists:\n",
    "        insert_existing_table( bqclient, fact_table_path, df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
